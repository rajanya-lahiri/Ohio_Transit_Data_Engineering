**Ohio Transit Data Engineering ğŸš**

**ğŸ“Œ Overview**

End-to-end data engineering project on Ohioâ€™s public transit (COTA GTFS). Built a pipeline that transforms raw CSVs into analytics-ready datasets and dashboards for insights.

â¸»

**âš™ï¸ Tech Stack**
	â€¢	Airflow â€“ Orchestration
	â€¢	BigQuery â€“ Cloud data warehouse
	â€¢	dbt â€“ Transformations & tests
	â€¢	Docker + Astro CLI â€“ Local dev
	â€¢	Looker Studio â€“ Visualization

â¸»

**ğŸš€ What It Does**
	â€¢	Automates ingestion of GTFS transit files
	â€¢	Cleans & models data into staging + marts layers
	â€¢	Delivers insights on:
	â€¢	Busiest stops & routes
	â€¢	Service hours (weekday vs weekend)
	â€¢	Headway variability (frequency of service)

â¸»

**ğŸ“Š Outcomes**
	â€¢	Turned static CSVs into live dashboards
	â€¢	Reduced manual reporting with automated ETL
	â€¢	Created a scalable pipeline replicable for other cities

â¸»

**ğŸ› ï¸ How to Run**
	1.	Clone repo & configure GCP credentials
	2.	Run Airflow with astro dev start
	3.	Trigger DAGs â†’ Data lands in BigQuery
	4.	Run dbt run â†’ Transformations
	5.	Connect Looker Studio â†’ Dashboards ready

â¸»

**ğŸ“‚ Project Architecture**

Raw GTFS Data (COTA CSVs)  
        â”‚  
        â–¼  
 Google Cloud Storage (GCS)  
        â”‚  
        â–¼  
Apache Airflow DAGs â”€â”€â”€â”€â”€â”€â–º BigQuery (raw â†’ staging â†’ marts)  
                                â”‚  
                                â–¼  
                          dbt Models & Tests  
                                â”‚  
                                â–¼  
                         Looker Studio Dashboard  
						 
â¸»

**ğŸ“– Learnings**
	â€¢	Hands-on with cloud data engineering
	â€¢	Designing raw â†’ staging â†’ marts pipelines
	â€¢	Turning messy GTFS data into usable insights
