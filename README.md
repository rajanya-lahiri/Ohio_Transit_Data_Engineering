# Ohio Transit Data Engineering ğŸš  

## ğŸ“Œ Overview  
End-to-end **data engineering project** on Ohioâ€™s public transit (COTA GTFS).  
Built a pipeline that transforms raw CSVs into **analytics-ready datasets** and **dashboards** for insights.  

---

## âš™ï¸ Tech Stack  
- **Airflow** â€“ Orchestration  
- **BigQuery** â€“ Cloud data warehouse  
- **dbt** â€“ Transformations & tests  
- **Docker + Astro CLI** â€“ Local dev  
- **Looker Studio** â€“ Visualization  

---

## ğŸš€ What It Does  
- Automates ingestion of GTFS transit files  
- Cleans & models data into staging + marts layers  
- Delivers insights on:  
  - Busiest stops & routes  
  - Service hours (weekday vs weekend)  
  - Headway variability (frequency of service)  

---

## ğŸ“Š Outcomes  
- Turned static CSVs into live dashboards  
- Reduced manual reporting with automated ETL  
- Created a scalable pipeline replicable for other cities  

---

## ğŸ› ï¸ How to Run  
1. **Clone repo & configure GCP credentials**  
   ```bash
   git clone https://github.com/rajanya-lahiri/Ohio_Transit_Data_Engineering.git
   cd Ohio_Transit_Data_Engineering
2. **Start Airflow locally**
   astro dev start
3. **Trigger DAGs** â†’ Loads raw â†’ staging data into BigQuery
4. **Run dbt models**
    cd dbt
	dbt run
	dbt test
5. **Connect Looker Studio** â†’ Build dashboards from marts dataset

---

## ğŸ“– Learnings
	â€¢	Hands-on with cloud data engineering
	â€¢	Designing raw â†’ staging â†’ marts pipelines
	â€¢	Turning messy GTFS data into usable insights

---

## ğŸ“ˆ Future Improvements
	â€¢	Automate daily GTFS updates
	â€¢	Add real-time bus tracking feeds
	â€¢	Expand analysis to multiple cities
